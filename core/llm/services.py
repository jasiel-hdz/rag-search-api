from openai import OpenAI
from fastapi import HTTPException
from config import get_settings

settings = get_settings()


class LLMService:
    """
    Service to interact with OpenAI API.
    Handles response generation using language models.
    """
    
    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
    
    def generate_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """
        Generates a response using OpenAI model.
        
        Args:
            prompt: User message or prompt
            temperature: Temperature for generation (0.0-2.0). Default: 0.7
            max_tokens: Maximum number of tokens in response. Default: 1000
            
        Returns:
            Response generated by the model
            
        Raises:
            HTTPException: If there's an error communicating with OpenAI
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Extract response content
            return response.choices[0].message.content.strip()
            
        except HTTPException:
            raise
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Error generating response with OpenAI: {str(e)}"
            )


# Singleton service instance
llm_service = LLMService()

