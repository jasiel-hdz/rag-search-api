from openai import OpenAI
from fastapi import HTTPException
from config import get_settings

settings = get_settings()


class LLMService:
    """Service to interact with OpenAI API"""
    
    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
    
    def generate_response(self, prompt: str) -> str:
        """
        Generate a response using the OpenAI model
        
        Args:
            prompt: The user's message
            
        Returns:
            The response generated by the model
            
        Raises:
            HTTPException: If there's an error communicating with OpenAI
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            # Extract the response content
            return response.choices[0].message.content.strip()
            
        except HTTPException:
            raise
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Error generating response with OpenAI: {str(e)}"
            )


# Singleton service instance
llm_service = LLMService()  

